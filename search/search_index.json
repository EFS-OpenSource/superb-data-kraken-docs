{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"A data platform for everyone"},{"location":"#superb-data-kraken","title":"SUPERB DATA KRAKEN","text":"<p>An automated data platform to capture, process, analyze and present your data</p> <p>From your data to your insights in interactive dashboards within minutes. Automation of knowledge extraction through customizable workflows.</p> <p>SDK is more than just a business intelligence platform as a data platform. Reacquaint yourself with your data and learn how to leverage it further!  </p>"},{"location":"#what-is-superb-data-kraken","title":"What is Superb Data Kraken?","text":"<p>Superb Data Kraken (SDK) is a generalized, multi-functional data platform designed to offer a versatile and rich set of tools to easily handle the most common tasks encountered in data management.</p> <p>This includes the collection, storage, processing, access management, and analysis of various kinds of data and its metadata. Its fundamental design is a modular microservice architecture composed of loosely coupled components. These can be combined in numerous ways to fulfill the unique needs of each deployment, facilitating the smooth integration of additional modules as required.</p> <p>Where appropriate, existing open-source solutions are being used. When these are not suitable, custom services have been developed.</p>"},{"location":"#how-does-it-work","title":"How does it work?","text":"<p>SDK is divided into different components that can be combined according to your preferences. It all starts with data upload. In this process, data is processed through a secure area and transferred to the desired storage location.</p> <p>During the upload, metadata is captured and processed in a search engine. If needed, this data can be made fully searchable based on content. Automated analysis takes place through workflows, which can be generated and customized as per requirements and references.</p> <p>The results are then stored in appropriate databases and visualized through interactive dashboards. Users can access the data and the insights derived from it. Of course, access permissions can be managed and personalized by the admin at any time.  </p> <p> </p>"},{"location":"#what-is-beyond-your-data","title":"What is beyond your data?","text":"<p>Everyone has the feeling of knowing their data and consequently makes decisions based on this feeling. It would be better to bring the data together and analyze it to turn feelings into facts and let them speak. We're here to help you build the infrastructure for your data.</p> <p>The goal is to generate knowledge from corporate data using custom processing pipelines. Make decisions based on facts and lead your business into a successful and sustainable future with a data strategy.</p>"},{"location":"#documentation","title":"Documentation","text":"<p>Architecture</p> <p>User Guide</p> <p>Operator Guide</p>"},{"location":"#features","title":"Features","text":"<p>The Superb Data Kraken provides services that enable data to be handled on a generic scale. This includes the following features:</p> <ul> <li>User-Management</li> <li>Role-Based-Access-Control</li> <li>Storage</li> <li>Metadata-Management</li> <li>Search</li> <li>Analysis</li> <li>Visualization</li> <li>Automation</li> </ul> <p>However, Superb Data Kraken can be individually extended according to your requirements. Just reach out to us via  sdk@efs-techhub.com.</p>"},{"location":"#references","title":"References","text":"<p>UI</p> <p>organizationmanager</p> <p>accessmanager</p> <p>commons</p> <p>logging</p> <p>avro-models</p> <p>metadata-service</p> <p>worker</p> <p>storagemanager</p> <p>search-service</p> <p>ingest</p>"},{"location":"architecture/","title":"Components","text":"<p>The following diagram provides a brief overview of all the services that make up the Superb Data Kraken. Services that are self-implemented are marked blue, while open-source solutions are marked grey.</p> <p>All services and modules run on a Kubernetes cluster.</p> <p></p> <p>At this point, it should be noted that all communication within the kubernetes cluster takes place via http as far as possible (the communication between frontend and backend is of course ssl-encrypted). The representations in the following chapters contain the respective protocol in square brackets. Some communication paths are specified more than once (\"<code>[http/https]</code>\"), this is due to the fact that it depends on how the communication is established ( cluster-internal or -external/frontend).</p> <p>Our Services are implemented with Spring Boot and therefore use Spring MVC to handle REST requests and automatically maps incoming JSON or XML payloads to predefined Java models using Jackson or JAXB. The mapping is based on the field names and types of the Java model and the payload.</p>"},{"location":"architecture/#organizationmanager","title":"Organizationmanager","text":"<p>Superb Data Kraken is organized in organizations and spaces, where a space represents a use-case and an organization packages use-cases. The Organizationmanager is a service of the Superb Data Kraken for the management of organizations and spaces.</p> <p>On a technical level, an organization corresponds to a Storage Account, whereas a space corresponds to a Container. Each organization has a dedicated \" Container\" called 'loadingzone', which serves as an intermediate store for incoming data. After processing, this data will be moved to the main-storage ( target-space) - however this is out of this service's scope (see ingest).</p> <p>Each space can handle so-called capabilities, which specify which capabilities a space should have:</p> <ul> <li>STORAGE: dedicated storage-area for data - \"suppliers\" will be able to upload data</li> <li>METADATA: dedicated \"measurement\"-Index in opensearch for storing metadata to the massdata</li> <li>ANALYSIS: provides capabilities in order to generate custom analysis</li> </ul> <p>The organizationmanager calls the defined services to create the complete context infrastructure when creating an organization or space.</p> <p></p> <p>In addition to organizations and spaces, the organizationmanager manages the authorization for these. For information on which roles exist and what they are authorized for, please refer to the roles/rights concept.</p> <p></p> <p>If users require authorization to a certain organization/space, they can request access (\"<code>UserRequest</code>\") with the desired roles. An administrator (or owner) can then grant permission or deny it. An administrator (or owner) can also directly grant permissions to users.</p> <p></p>"},{"location":"architecture/#metadata","title":"Metadata","text":"<p>Handles opensearch-structure (roles, rolesmappings, tenants...) and provides endpoints for indexing and handling application-indizes.</p> <p>When creating an organization, metadata-Service will provide a tenant, as well as dedicated roles and rolesmappings for organization-level roles.</p> <p></p> <p>When creating a space, metadata-Service will provide a tenant, as well as dedicated roles and rolesmappings for space-level roles and an index <code>&lt;orga&gt;_&lt;space&gt;_measurements</code> (including an alias <code>measurements</code>).</p> <p></p> <p>Note</p> <p>Only trustee can write to index, however also a supplier can create a document! This is due to the fact, that a supplier would be able to overwrite a document \"empty\" and thus delete a document (which would clash with the roles-/right-concept). Therefore a serviceaccount is used.</p> <p></p> <p>To enable users to create their own analyses, it is possible to create their own analysis indices (application indices) via the metadata service. These analysis indices are given a name according to the following scheme: <code>&lt;orga&gt;_&lt;space&gt;_analysis_&lt;a_custom_name&gt;</code>.</p> <p></p>"},{"location":"architecture/#storagemanager","title":"Storagemanager","text":"<p>Handles storageorganization (in Azure: creates StorageAccount for organization and Container for space; in S3: creates matching prefixes).</p> <p>Here is, how data is being stored in Superb Data Kraken:</p> <p></p> <p>The reason why there is a dedicated loading zone is that it gives the user the possibility to provide subsequent deliveries to his dataset. Also, this way it can be ensured that all data that has been indexed is complete and valid (see Ingest).</p> <p>The following illustration represent the storagemanagement as it is being handled in Azure.</p> <p></p>"},{"location":"architecture/#accessmanager","title":"Accessmanager","text":"<p>Provides endpoints for generating shared access signatures, as well as an endpoint for signalling, that the upload is complete (\"commit\").</p> <p>Note</p> <p>Endpoints for shared access signatures are only available, if Azure is the underlaying storage-provider.</p> <p>There are four different endpoints for generating according shared access signatures:</p> <ul> <li>Read</li> <li>Upload</li> <li>Delete</li> <li>Upload-main - this endpoint is only viable for trustees, it enables the user to upload directly to main-storage (passing loadingzone)</li> </ul> <p></p> <p>In order for subsequent processes to start without relying on conventions (\"meta.json\" uploaded last), a dedicated endpoint signals the completion of the upload. This sends a message to the Azure EventHub (\"accessmanager-commit\") to which the workflow engine can respond.</p> <p></p> <p>Note</p> <p>For downward-capability accessmanager has some endpoints for managing organizations and spaces. However this functionality is deprecated and will be removed.</p>"},{"location":"architecture/#ingest","title":"Ingest","text":"<p>The ingest describes how data is fed to the system. It consists of the following steps:</p> <ul> <li>basic-metadata-handling: if no meta.json is provided and the space is configured so generating of a basic-metadata-set is supported, this handler will do the   job</li> <li>anonymize: currently not implemented</li> <li>enrichment: currently not implemented</li> <li>validate: currently not implemented</li> <li>move-data: moves data from loadingzone to the main-storage</li> <li>metadata-index: indexes metadata to <code>&lt;orga&gt;_&lt;space&gt;_measurements</code>-index</li> </ul> <p>Note</p> <p>For simplicity-reasons the following illustration is restricted on azure-specific implementation.</p> <p>Although a serviceaccount-token is initially created by the ingest, the index endpoint of the metadata service is invoked with a user-token to take advantage of the request validation of the metadata service (knowing that the metadata-service also switches to a serviceaccount context).</p> <p></p> <p>Note</p> <p>There are considerations of defining a custom ingest-process for a space.</p>"},{"location":"architecture/#search","title":"Search","text":"<p>The search-service wraps opensearch in a user-friendly manner. It provides endpoints to enable listing all available properties including the possible search-operators (e.g. string-properties do not support <code>&lt;</code>, <code>&gt;</code>). Some properties might be available as result-property however not as criteria (<code>enabled</code>), which is also taken into consideration.</p> <p></p> <p></p>"},{"location":"architecture/roles-and-rights/","title":"Roles and rights","text":""},{"location":"architecture/roles-and-rights/#roles-rights","title":"Roles &amp; Rights","text":"<p>Rights and permissions are handled on the level of organizations and spaces, which are abstractions relating to customers (organization) and their projects (spaces). Each organization has one or multiple spaces, and each space has only one organization it is related to. Permissions are granted via roles for organizations and spaces separately. Users can be members of multiple organizations as well as multiple spaces by being assigned roles for their respective level of access.</p>"},{"location":"architecture/roles-and-rights/#roles","title":"Roles","text":"<p>Individual rights are grouped into roles as follows:</p>"},{"location":"architecture/roles-and-rights/#organization-level","title":"organization-Level","text":""},{"location":"architecture/roles-and-rights/#information-owner-owner","title":"Information owner (=Owner)","text":"<ul> <li>Approver for the assignment of rights, cf. rights assignment process at the organization level.   The information owner is a person who is responsible for maintaining the confidentiality of certain information.   He/she is responsible for the contents of the organization (responsibility in case of legal violations or similar).</li> </ul> <p>The role of the information owner must be filled (by a representative or successor), and it is his responsibility to ensure that the role of the information owner is filled.</p>"},{"location":"architecture/roles-and-rights/#access","title":"Access","text":"<ul> <li>basic-authorization to access data</li> <li>in order to access data, dedicated space-permissions are required</li> </ul>"},{"location":"architecture/roles-and-rights/#admin","title":"Admin","text":"<ul> <li>representative of the owner</li> <li>is authorized by the owner to perform administrative activities</li> </ul>"},{"location":"architecture/roles-and-rights/#trustee","title":"Trustee","text":"<ul> <li>is authorized to handle Dashboards on an organization-leveled basis</li> </ul>"},{"location":"architecture/roles-and-rights/#space-level","title":"space-Level","text":""},{"location":"architecture/roles-and-rights/#information-owner-owner_1","title":"Information owner (=Owner)","text":"<ul> <li>Approver for the assignment of rights, cf. rights assignment process at the space level.   The information owner is a person who is responsible for maintaining the confidentiality of certain information.   He/she is responsible for the contents of the space (responsibility in case of legal violations or similar).</li> </ul> <p>The role of the information owner must be filled (representative or successor), that the role of the information owner and the data keeper (trustee) are filled is his responsibility.</p>"},{"location":"architecture/roles-and-rights/#supplier","title":"Supplier","text":"<ul> <li>may create new data</li> <li>may read existing data on the loading zone</li> </ul>"},{"location":"architecture/roles-and-rights/#trustee_1","title":"Trustee","text":"<ul> <li>may create new data and edit or delete them</li> </ul>"},{"location":"architecture/roles-and-rights/#user","title":"User","text":"<ul> <li>may read and download information from the storage</li> </ul>"},{"location":"architecture/roles-and-rights/#a-closer-look","title":"A closer Look","text":"<p>Taking the preceding explanations into account, the following authorization-matrix results:</p> organization Owner Admin Access Trustee Authorization (organization-Level) Read Members x x Get Authorization x x Edit Members x x Edit Authorization x x Authorization (space-Level) Read Members x x Get Authorization x x Edit Members x x Edit Authorization x x Userrequests (organization-Level) Create Userrequest <sup>1</sup> List Userrequest <sup>2</sup> x x Edit Userrequest x x Userrequests (space-Level) Create Userrequest <sup>1</sup> List Userrequest <sup>2</sup> x x Edit Userrequest x x Owners Get Owners x x x x Edit Owners x organization-Management Create organization <sup>3</sup> Update organization x x Get organization x x x x space-Management Get space x x x <sup>4</sup> Create space x x Edit space x x Mark space for deletion <sup>5</sup> x x Delete Space6 <sup>6</sup> Dashboards Create Dashboards x x space Owner <sup>7</sup> User Supplier Trustee Authorization (space-Level) Read Members x Get Authorization x Edit Members x Edit Authorization x Userrequests (space-Level) Create Userrequest <sup>1</sup> List Userrequest <sup>2</sup> x x Edit Userrequest x x Owners Get Owners x x x x Edit Owners x space-Management Get space x x x x Edit space x Mark space for deletion <sup>5</sup> x x Delete Space6 <sup>6</sup> Measurement data (Loadingzone) Read/Download x x Edit/Upload x x Delete x x Measurement data Read/Download x x x Edit/Upload x x Delete x Metadata Read x x x Edit <sup>8</sup> x Delete x Analysis Create Application-Index x Delete Application-Index <sup>9</sup> x Read/use Analysis x x x Create Analysis <sup>10</sup> x <p>Special-case: PUBLIC-Confidentiality - besides the standard roles there are roles \"org_all_public\" and \" spc_all_public\", which every user has. These roles authorize read access in the corresponding area, while the corresponding organization/space-roles are available for write access.</p> organization all_public organization-Management Get organization x space all_public space-Management Get space x Measurement data Read/Download x Metadata Read x <ol> <li> <p>Userrequests can be created by any user\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Every user can list their own UserRequests\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>Creating Organizations is currently only supported for global admins, however we plan to introduce global organization admins for this purpose\u00a0\u21a9</p> </li> <li> <p>Read rights only if permissions exist on the corresponding space \u21a9</p> </li> <li> <p>To avoid unintentional data loss, spaces are not deleted immediately but only after a certain grace period - see <sup>6</sup> \u21a9\u21a9</p> </li> <li> <p>Deleting a space immediately is reserved for global administrators\u00a0\u21a9\u21a9\u21a9</p> </li> <li> <p>An owner is assigned all space roles at the beginning of his status, which is why he has full access in the space context. This representation therefore only contains those rights that an owner has in his capacity as owner.\u00a0\u21a9</p> </li> <li> <p>Since deletion in Opensearch can also be interpreted as \"empty overwrite\", these permissions are summarized\u00a0\u21a9</p> </li> <li> <p>Here the creator of the index must be considered\u00a0\u21a9</p> </li> <li> <p>However in order to create a dashboard, \"orga-trustee\"-permission is required\u00a0\u21a9</p> </li> </ol>"},{"location":"developer-guide/CONTRIBUTING/","title":"Contributing to the Superb Data Kraken (SDK)","text":"<p>The Superb Data Kraken (SDK) operates an open contribution model where everyone may contribute in development, reviewing and testing. We value the contributions of our community members, and your help is essential to making this project even better.  This document presents guidelines and best practices on how you can contribute to the project.</p>"},{"location":"developer-guide/CONTRIBUTING/#introduction","title":"Introduction","text":"<p>Contributions in form of tests, peer reviews and code are welcome and needed. Testing and reviewing tasks are highly appreciated and a good starting point to familiarize oneself with the project. In addition, there are issues with the good first issue label as a starting point. If you are interested in an issue, it is good to leave a comment to make sure the issue is still applicable and to inform others that you plan to address it.</p> <p>In addition to contributors, there are repository maintainers who are responsible for merging pull requests and moderation.</p> <p>When contributing, please follow the general Contribution guidelines to Open Source Projects, as long as not stated otherwise below.</p>"},{"location":"developer-guide/CONTRIBUTING/#getting-started","title":"Getting Started","text":"<p>To contribute to Superb Data Kraken, follow these steps:</p> <ol> <li>Fork the repository on GitHub.</li> <li>Clone your forked repository to your local machine.</li> </ol> <p>Please follow the documentation carefully on how to set up the development environment and provide tests.</p>"},{"location":"developer-guide/CONTRIBUTING/#issues","title":"Issues","text":""},{"location":"developer-guide/CONTRIBUTING/#create-a-new-issue","title":"Create a new issue","text":"<p>If you encounter a bug or would like to request a new feature, please open an issue on our issue tracker and follow the provided template.</p>"},{"location":"developer-guide/CONTRIBUTING/#solve-an-issue","title":"Solve an issue","text":"<p>Scan through our issue tracker to find one that interests you. </p>"},{"location":"developer-guide/CONTRIBUTING/#contributing-code","title":"Contributing Code","text":"<p>The workflow to submit changes is as follows: 1. Create a topic branch \\ Check out a new branch based on the development branch (see branching conventions) according to the branch naming conventions. Use one branch per fix / feature. 2. Contribute code \\ To maintain consistency in our codebase, please follow coding standards and best practices and legal compliance. 3. Commit patches and when ready, push to the remote branch 4. Squash commits to a single feature commit with a meaningful commit message 5. Create a Pull request (PR) to submit your changes for review. Fill out the pull request template and perform a self-review.  6. Address Feedback during peer review - You can add more commits to your pull request by committing them locally and pushing to your fork. - Please reply to any review comments before your pull request is merged. - If there is outstanding feedback, and you are not actively working on it, your pull request may be closed.</p>"},{"location":"developer-guide/CONTRIBUTING/#review-process","title":"Review Process","text":"<p>Anyone may participate in peer review and comment pull requests. The central criteria follow the self-review checklist. Please comment with the checklist items that you checked and, if a point of the checklist is not OK in your opinion, include an explanation why.</p>"},{"location":"developer-guide/CONTRIBUTING/#decision-process","title":"Decision Process","text":"<p>The decision whether a pull request is merged into develop/main rests with the project merge maintainers and takes the review into consideration.</p>"},{"location":"developer-guide/CONTRIBUTING/#legal-compliance","title":"Legal Compliance","text":""},{"location":"developer-guide/CONTRIBUTING/#avoiding-copied-code","title":"Avoiding Copied Code","text":"<p>We value original and legally compliant contributions to this project. To ensure that we respect intellectual property rights and maintain compliance with software licenses, we kindly request that all contributors refrain from checking in copied code, including code from third-party sources, without the appropriate permissions or licenses.</p>"},{"location":"developer-guide/CONTRIBUTING/#licensing","title":"Licensing","text":"<p>By contributing to this project, you agree that your contributions will be subject to the project's license and will comply with the following:</p> <ul> <li>Any code you submit must be your original work, or you must have the necessary permissions to contribute it.</li> <li>If you include code from other sources (e.g., libraries, frameworks, or open-source projects), ensure that the code is properly attributed, and its licensing terms are compatible with this project's licensing.</li> <li>Please contain the standard SDK Apache License 2.0 header in all (new) files.</li> </ul>"},{"location":"developer-guide/CONTRIBUTING/#reporting-copyright-violations","title":"Reporting Copyright Violations","text":"<p>If you suspect that any contributed code violates copyright or licensing agreements, please promptly notify the project maintainers by opening an issue on Issue Tracker or contacting us.</p>"},{"location":"developer-guide/CONTRIBUTING/#conclusion","title":"Conclusion","text":"<p>Thank you for considering contributing to Superb Data Kraken! Your contributions are greatly appreciated, and they help make this project better for everyone. Get started today and be part of our open-source community!</p>"},{"location":"developer-guide/branching-guidelines/","title":"Branching guidelines","text":""},{"location":"developer-guide/branching-guidelines/#branching-guidelines","title":"Branching Guidelines","text":""},{"location":"developer-guide/branching-guidelines/#branching","title":"Branching","text":"<p>The <code>main</code>-branch portraits a stable, tested status. </p> <p>The <code>develop</code>- branch is based on the <code>main</code>-branch for single features and will be merged into the main with each new version. </p> <p>Each new feature will be developed in a <code>feature</code>-Branch, which holds each commit for its development. After the development a review will occur, if everything is fine, it will be merged into the <code>develop</code>-Branch. Please consider naming conventions.</p> <p>If there are any bugs detected in the <code>main</code>-Branch, fixes should be made in a <code>hotfix</code>-Branch. This will be created based on the main-Branch and will be merged into develop and subsequently into the main. Please consider naming.</p> <p></p>"},{"location":"developer-guide/branching-guidelines/#naming","title":"Naming","text":"branch-type example description feature f/123-short-description branch contains implementation of feature 123, with a short description hotfix h/566-short-description branch contains fix of bug 566, with a short description develop develop a repository's development-branch main main a repository's main-branch <p>In order to provide a fast identification, whether a feature or bugfix is being handled in a branch, naming-conventions are required.</p> <p>This also enables filtering via git e.g.</p> <p><code>git branch --list 'f*'</code></p> <p>Also, it enables a CI/CD-tool change automated processing, which can lead to varying actions based on the branch-type.</p>"},{"location":"developer-guide/git-commands/","title":"Git commands","text":""},{"location":"developer-guide/git-commands/#git-commands","title":"Git Commands","text":""},{"location":"developer-guide/git-commands/#create-topic-branch","title":"Create topic branch","text":"<p>Check out a new branch based on the development branch according to the branch naming conventions:</p> <p><pre><code>$ git checkout -b BRANCH_NAME\n</code></pre>   If you get an error, you may need to fetch first by using   <pre><code>$ git remote update &amp;&amp; git fetch\n</code></pre></p>"},{"location":"developer-guide/git-commands/#commit-patches","title":"Commit patches","text":"<p>To commit your changes to your local repository   <pre><code>$ git commit\n</code></pre></p>"},{"location":"developer-guide/git-commands/#push-to-the-remote-branch","title":"Push to the remote branch","text":"<p>To push to the remote branch       <pre><code>$ git push origin BRANCH_NAME\n</code></pre></p>"},{"location":"developer-guide/git-commit-guidelines/","title":"Git commit guidelines","text":""},{"location":"developer-guide/git-commit-guidelines/#git-commit-guidelines","title":"Git Commit Guidelines","text":"<p>Please make sure that the (squashed) git commits that are to be merged within a pull request   - are (if applicable) atomic, i.\\,e. do not mix code reformatting, code moves and code changes or commit multiple features at once.   - are clean and complete, i.\\,e. the code builds without errors or warnings or test failures, and provides documentation and tests for the new feature.   - have git message that explains what you've done and why (see below)</p>"},{"location":"developer-guide/git-commit-guidelines/#commit-messages","title":"Commit-Messages","text":"<p>A git commit message should explain concisely what was done and why, with justification and reasoning, and follow the seven rules of a great Git commit message 1. Separate subject from body with a blank line 2. Limit the subject line to 50 characters 3. Capitalize the subject line 4. Do not end the subject line with a period 5. Use the imperative mood in the subject line 6. Wrap the (optional) body at 72 characters 7. Use the (optional) body to explain what and why vs. how</p> <p>If the title alone is self-explanatory (like \"Correct typo in CONTRIBUTING.md\"), a single title line is sufficient. Do not make any username <code>@</code> mentions.</p> <p>This structure provides the possibility to automatically generate changelogs. If a particular commit references an issue, please add the reference, e.g. <code>refs #1234</code> or <code>fixes #1234</code> or <code>closes #1234</code>, as this provides the possibility to automatically close the corresponding issue when the pull request is merged.</p> <p>In order to adhere to this structure, it is helpful to use a commit-template. Please edit your .gitconfig as follows:</p> <p><pre><code>[commit]\ntemplate = ~/.gitmessage\n</code></pre> and provide a .gitmessage in your user-home.</p> <p>Here is an example of a .gitmessage-file in SDK-environment: <pre><code>&lt;type&gt;[optional scope]: &lt;summary, max 50 characters&gt;\n\n[optional body]\n\n[optional footer(s)]\n</code></pre></p> <p>And an example commit message:</p> <pre><code>fix: Prevent racing of requests\n\nIntroduce a request id and a reference to latest request. Dismiss\nincoming responses other than from latest request.\n\nRemove timeouts which were used to mitigate the racing issue but are\nobsolete now.\n\nRefs: #123\n</code></pre> <p>The following types shall be used: - fix: a commit of the type fix patches a bug in your codebase (this correlates with PATCH in Semantic Versioning). - feat: a commit of the type feat introduces a new feature to the codebase (this correlates with MINOR in Semantic Versioning). - BREAKING CHANGE: a commit that has a footer 'BREAKING CHANGE: ', or appends a ! after the type/scope, introduces a breaking API change (correlating with MAJOR in Semantic Versioning). A BREAKING CHANGE can be part of commits of any type. - types other than 'fix' and 'feat' are allowed, for example @commitlint/config-conventional (based on the Angular convention) recommends build:, chore:, ci:, docs:, style:, refactor:, perf:, test:, and others. - footers other than 'BREAKING CHANGE: ' may be provided and follow a convention similar to git trailer format. <p>For more information, see conventional commits.</p>"},{"location":"developer-guide/pull-request-template/","title":"Pull request template","text":""},{"location":"developer-guide/pull-request-template/#description","title":"Description","text":"<p>Please include a summary of the changes and the related issue. Please also include relevant motivation and context. </p> <p>Dependencies: List any dependencies that are required for this change.</p> <p>Fixes # (issue)</p>"},{"location":"developer-guide/pull-request-template/#type-of-change","title":"Type of change","text":"<p>Please delete options that are not relevant.</p> <ul> <li>[ ] Bug fix (non-breaking change which fixes an issue)</li> <li>[ ] New feature (non-breaking change which adds functionality)</li> <li>[ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)</li> <li>[ ] This change requires a documentation update</li> </ul>"},{"location":"developer-guide/pull-request-template/#how-has-this-been-tested","title":"How Has This Been Tested?","text":"<p>Please describe the tests that you ran to verify your changes. </p> <ul> <li>[ ] Test A</li> <li>[ ] Test B</li> </ul>"},{"location":"developer-guide/pull-request-template/#self-review","title":"Self review:","text":"<p>Please check that your changes...</p> <ul> <li>[ ] meet the user experience and documentation or architecture design report, if there is one </li> <li>[ ] follow a consistent and well-suited approach</li> <li>[ ] follow the style guidelines of the project</li> <li>[ ] generate no new warnings, no regressions and be compatible with existing features</li> <li>[ ] pass sonarlint checks</li> <li>[ ] include code and user documentation updates, if applicable</li> <li>[ ] include tests that prove the fix is effective or the feature works</li> <li>[ ] pass new and existing unit tests locally</li> <li>[ ] include the standard SDK Apache License 2.0 header in all new files</li> <li>[ ] are not blocked from merge (i.e. do not depend on unmerged or unpublished features in dependent modules)</li> </ul>"},{"location":"operator-manual/installation/","title":"Installing Superb Data Kraken","text":"<p>The Superb Data Kraken consists of multiple services which combine to the most amazing Data Kraken </p> <p>Note</p> <ul> <li>These instructions assume, that the prerequesits are already met.</li> <li>You might want to manage your configuration globally, as many properties are required by various services.</li> </ul>"},{"location":"operator-manual/installation/#organizationmanager","title":"Organizationmanager","text":"<p>To get an up and running instance of the organizationmanager the following steps are required:</p> <ul> <li>provide a PostgreSQL-database, configure via the following properties:<ul> <li>in config-map.yml adjust <code>$(DATABASE_SERVER)</code> and <code>$(ORGAMANAGER_DATABASE)</code> accordingly</li> <li>in provided-secrets.yml adjust <code>DATABASE_PASSWORD</code> and <code>DATABASE_USER</code> (in the form of <code>&lt;USER&gt;@&lt;DATABASE_SERVER&gt;</code>) accordingly</li> </ul> </li> <li>provide a Kafka-instance with the following topic: <code>space-deleted</code> (or any other topic configured as property <code>organizationmanager.kafka.topic.space-deleted</code>), configure via the following properties:<ul> <li>in provided-secrets.yml adjust <code>KAFKA_SASL_JAAS_CONFIG</code> (in the form of <code>org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://&lt;KAFKA_BROKER&gt;/;SharedAccessKeyName=&lt;KEY_NAME&gt;;SharedAccessKey=&lt;KEY&gt;\";</code>) accordingly</li> </ul> </li> <li>provide the following libraries locally:<ul> <li>superb-data-kraken-logging</li> <li>superb-data-kraken-common</li> <li>superb-data-kraken-storage-schemas</li> </ul> </li> <li>build a Docker-image to the container-registry of your liking. We provided you with 2 options:<ul> <li>with additional logging to Azure Application Insights (Dockerfile)</li> <li>and without Azure Application Insights (Dockerfile-no-appinsights)</li> </ul> </li> </ul> <p>If you choose to use the Docker-image with additional logging to Azure Application Insights, you need to provide extra properties in config-map.yml: </p> <ul> <li><code>APP_INSIGHTS_CONNECTION_STRING</code> the Connection-String of your Azure Application Insights instance</li> <li><code>APP_INSIGHTS_INSTRUMENTATION_KEY</code> the Instrumentation-Key of your Azure Application Insights instance</li> </ul> <p>For additional configuration, please consider these properties within the kubernetes-folder:</p> <ul> <li><code>postfix</code> a postfix you might want to add to your servides ( should be consistent across your installation as cluster-interal domains are predifined with this postfix )</li> <li><code>REALM</code> the specific realm set up with the openid connect (oidc) provider</li> <li><code>CLIENT_ID</code> the client-id within defined realm (used for simplifying swagger-access)</li> <li><code>CLIENT_ID_CONFIDENTIAL</code> the id of the confidential client used for Service Account-access. This Service Account should have the following permissions:<ul> <li>get/update users in User-Management</li> <li>get/create/update/delete roles in User-Management</li> </ul> </li> <li><code>CLIENT_SECRET_CONFIDENTIAL</code> the secret of the confidential client used for Service Account-access</li> <li><code>LOG_LEVEL</code> the logging-level for organizationmanager</li> <li><code>CONTAINER_REGISTRY</code> the container-registry that stores the Docker-image</li> <li><code>tagVersion</code> the tag-version of the Docker-image</li> <li><code>DOMAIN</code> the domain organizationmanager should be available at</li> <li><code>KAFKA_BOOTSTRAP_SERVER</code> the Kafka-Bootstrap-Server</li> </ul>"},{"location":"operator-manual/installation/#storagemanager","title":"Storagemanager","text":"<p>To get an up and running instance of the storage-manager the following steps are required:</p> <ul> <li>provide the following libraries locally:<ul> <li>superb-data-kraken-logging</li> <li>superb-data-kraken-common</li> </ul> </li> <li>build a Docker-image to the container-registry of your liking. We provided you with 2 options:<ul> <li>with additional logging to Azure Application Insights (Dockerfile)</li> <li>and without Azure Application Insights (Dockerfile-no-appinsights)</li> </ul> </li> </ul> <p>If you choose to use the Docker-image with additional logging to Azure Application Insights, you need to provide extra properties in config-map.yml: </p> <ul> <li><code>APP_INSIGHTS_CONNECTION_STRING</code> the Connection-String of your Azure Application Insights instance</li> <li><code>APP_INSIGHTS_INSTRUMENTATION_KEY</code> the Instrumentation-Key of your Azure Application Insights instance</li> </ul> <p>For additional configuration, please consider these properties within the kubernetes-folder:</p> <ul> <li><code>postfix</code> a postfix you might want to add to your servides ( should be consistent across your installation as cluster-interal domains are predifined with this postfix )</li> <li><code>REALM</code> the specific realm set up with the openid connect (oidc) provider</li> <li><code>CLIENT_ID</code> the client-id within defined realm (used for simplifying swagger-access)</li> <li><code>LOG_LEVEL</code> the logging-level for storage-manager</li> <li><code>CONTAINER_REGISTRY</code> the container-registry that stores the Docker-image</li> <li><code>tagVersion</code> the tag-version of the Docker-image</li> <li><code>DOMAIN</code> the domain storage-manager should be available at</li> <li><code>RESOURCE_GROUP</code> the resource-group in Azure the storage-principal is permitted to (see Azure Subscription)</li> <li><code>AZURE_STORAGE_CLIENT_ID</code> application (client) ID of the storage-principal managed application (see Azure Subscription)</li> <li><code>AZURE_STORAGE_CLIENT_SECRET</code> application (client) secret of the storage-principal managed application (see Azure Subscription)</li> <li><code>AZURE_TENANT_ID</code> the tenant-id of the Azure Subscription</li> </ul>"},{"location":"operator-manual/installation/#accessmanager","title":"Accessmanager","text":"<p>To get an up and running instance of the accessmanager the following steps are required:</p> <ul> <li>provide a Kafka-instance with the following topic: <code>accessmanager-commit</code> (or any other topic configured as property <code>accessmanager.topic.upload-complete</code>), configure via the following properties:<ul> <li>in provided-secrets.yml adjust <code>KAFKA_SASL_JAAS_CONFIG</code> (in the form of <code>org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://&lt;KAFKA_BROKER&gt;/;SharedAccessKeyName=&lt;KEY_NAME&gt;;SharedAccessKey=&lt;KEY&gt;\";</code>) accordingly</li> </ul> </li> <li>provide the following libraries locally:<ul> <li>superb-data-kraken-logging</li> </ul> </li> <li>build a Docker-image to the container-registry of your liking. We provided you with 2 options:<ul> <li>with additional logging to Azure Application Insights (Dockerfile)</li> <li>and without Azure Application Insights (Dockerfile-no-appinsights)</li> </ul> </li> </ul> <p>If you choose to use the Docker-image with additional logging to Azure Application Insights, you need to provide extra properties in config-map.yml: </p> <ul> <li><code>APP_INSIGHTS_CONNECTION_STRING</code> the Connection-String of your Azure Application Insights instance</li> <li><code>APP_INSIGHTS_INSTRUMENTATION_KEY</code> the Instrumentation-Key of your Azure Application Insights instance</li> </ul> <p>For additional configuration, please consider these properties within the kubernetes-folder:</p> <ul> <li><code>postfix</code> a postfix you might want to add to your servides ( should be consistent across your installation as cluster-interal domains are predifined with this postfix )</li> <li><code>REALM</code> the specific realm set up with the openid connect (oidc) provider</li> <li><code>CLIENT_ID</code> the client-id within defined realm (used for simplifying swagger-access)</li> <li><code>LOG_LEVEL</code> the logging-level for accessmanager</li> <li><code>CONTAINER_REGISTRY</code> the container-registry that stores the Docker-image</li> <li><code>tagVersion</code> the tag-version of the Docker-image</li> <li><code>DOMAIN</code> the domain accessmanager should be available at</li> <li><code>RESOURCE_GROUP</code> the resource-group in Azure the storage-principal is permitted to (see Azure Subscription)</li> <li><code>AZURE_STORAGE_CLIENT_ID</code> application (client) ID of the storage-principal managed application (see Azure Subscription)</li> <li><code>AZURE_STORAGE_CLIENT_SECRET</code> application (client) secret of the storage-principal managed application (see Azure Subscription)</li> <li><code>AZURE_TENANT_ID</code> the tenant-id of the Azure Subscription</li> <li><code>KAFKA_BOOTSTRAP_SERVER</code> the Kafka-Bootstrap-Server</li> </ul>"},{"location":"operator-manual/installation/#metadata","title":"Metadata","text":"<p>To get an up and running instance of the metadata-service the following steps are required:</p> <ul> <li>provide a Kafka-instance with the following topics: <code>indexing-done</code> (or any other topic configured as property <code>metadata.topics.indexing-done-topic</code>) - will be triggered, once a new metadata-set is indexed - and <code>metadata-update</code> (or any other topic configured as property <code>metadata.topics.metadata-update-topic</code>) - will be triggered, once a new metadata-set is updated configure via the following properties:<ul> <li>in provided-secrets.yml adjust <code>KAFKA_SASL_JAAS_CONFIG</code> (in the form of <code>org.apache.kafka.common.security.plain.PlainLoginModule required username=\"$ConnectionString\" password=\"Endpoint=sb://&lt;KAFKA_BROKER&gt;/;SharedAccessKeyName=&lt;KEY_NAME&gt;;SharedAccessKey=&lt;KEY&gt;\";</code>) accordingly</li> </ul> </li> <li>provide the following libraries locally:<ul> <li>superb-data-kraken-logging</li> <li>superb-data-kraken-common</li> </ul> </li> <li>build a Docker-image to the container-registry of your liking. We provided you with 2 options:<ul> <li>with additional logging to Azure Application Insights (Dockerfile)</li> <li>and without Azure Application Insights (Dockerfile-no-appinsights)</li> </ul> </li> </ul> <p>If you choose to use the Docker-image with additional logging to Azure Application Insights, you need to provide extra properties in config-map.yml: </p> <ul> <li><code>APP_INSIGHTS_CONNECTION_STRING</code> the Connection-String of your Azure Application Insights instance</li> <li><code>APP_INSIGHTS_INSTRUMENTATION_KEY</code> the Instrumentation-Key of your Azure Application Insights instance</li> </ul> <p>For additional configuration, please consider these properties within the kubernetes-folder:</p> <ul> <li><code>postfix</code> a postfix you might want to add to your servides ( should be consistent across your installation as cluster-interal domains are predifined with this postfix )</li> <li><code>REALM</code> the specific realm set up with the openid connect (oidc) provider</li> <li><code>CLIENT_ID</code> the client-id within defined realm (used for simplifying swagger-access)</li> <li><code>CLIENT_ID_CONFIDENTIAL</code> the id of the confidential client used for Service Account-access. This Service Account should have the following permissions:<ul> <li>access to OpenSearch security-plugin (edit roles/rolesmappings/tenants)</li> <li>update all indices in OpenSearch</li> </ul> </li> <li><code>CLIENT_SECRET_CONFIDENTIAL</code> the secret of the confidential client used for Service Account-access</li> <li><code>LOG_LEVEL</code> the logging-level for metadata-service</li> <li><code>CONTAINER_REGISTRY</code> the container-registry that stores the Docker-image</li> <li><code>tagVersion</code> the tag-version of the Docker-image</li> <li><code>DOMAIN</code> the domain metadata-service should be available at</li> <li><code>ELASTICSEARCH_SERVICE</code> name of the kubernetes-service of the elasticsearch/opensearch-client-service</li> <li><code>ELASTICSEARCH_SECURITY_ENDPOINT</code> endpoint of the elasticsearch/opensearch security-plugin (might be <code>/_plugins/_security/api</code> - OpenSearch - or <code>/_opendistro/_security/api</code> - Elasticsearch)</li> </ul>"},{"location":"operator-manual/installation/#search","title":"Search","text":"<p>To get an up and running instance of the search-service no explicit steps are required.</p> <p>For configuration, please consider these properties within the kubernetes-folder:</p> <ul> <li><code>postfix</code> a postfix you might want to add to your servides ( should be consistent across your installation as cluster-interal domains are predifined with this postfix )</li> <li><code>REALM</code> the specific realm set up with the openid connect (oidc) provider</li> <li><code>CLIENT_ID</code> the client-id within defined realm (used for simplifying swagger-access)</li> <li><code>LOG_LEVEL</code> the logging-level for search</li> <li><code>CONTAINER_REGISTRY</code> the container-registry that stores the Docker-image</li> <li><code>tagVersion</code> the tag-version of the Docker-image</li> <li><code>DOMAIN</code> the domain search should be available at</li> <li><code>ELASTICSEARCH_SERVICE</code> name of the kubernetes-service of the elasticsearch/opensearch-client-service</li> </ul>"},{"location":"operator-manual/installation/#ingest","title":"Ingest","text":"<p>To get an up and running instance of the metadata-service the following steps are required:</p> <ul> <li>provide an EventSource for your Kafka-topic defined in AccessManager (<code>accessmanager-commit</code> or any other topic configured as property <code>accessmanager.topic.upload-complete</code>) - this EventSource is referenced by ingest-Sensor:<ul> <li>EventSource accessmanager-commit:     accessmanager-commit.eventsource.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: EventSource\nmetadata:\n  name: accessmanager-commit\nspec:\n  template:\n    affinity:\n      nodeAffinity:\n        preferredDuringSchedulingIgnoredDuringExecution:\n        - weight: 1\n          preference:\n            matchExpressions:\n            - key: agentpool\n              operator: In\n              values:\n              - userpool\n    container:\n      resources:\n        requests:\n          cpu: 20m\n          memory: 200Mi\n        limits:\n          cpu: 20m\n          memory: 200Mi\n  azureEventsHub:\n    accessmanager:\n      fqdn: sdk-eventhub-dev.servicebus.windows.net\n      sharedAccessKeyName:\n        name: azure-event-source\n        key: sharedAccessKeyName\n      sharedAccessKey:\n        name: azure-event-source\n        key: sharedAccessKey\n      hubName: accessmanager-commit\n</code></pre></li> </ul> </li> <li>build the Docker-images to the container-registry of your liking:<ul> <li>skipvalidation</li> <li>basicmetadata</li> <li>movedata</li> <li>metadataindex</li> </ul> </li> </ul> <p>For additional configuration, please consider these properties within the kubernetes-folder:</p> <ul> <li><code>postfix</code> a postfix you might want to add to your servides ( should be consistent across your installation as cluster-interal domains are predifined with this postfix )</li> <li><code>CONTAINER_REGISTRY</code> the container-registry that stores the Docker-image</li> <li><code>tagVersion</code> the tag-version of the Docker-image</li> <li><code>DOMAIN</code> the domain oidc-provider is available at</li> <li><code>SKIP_VALIDATE_ORGANIZATIONS</code> Names of organization for which the validation-tree (<code>basicmetadata</code>, <code>anonymize</code>, <code>enrichment</code> and <code>validate</code>) should be skipped</li> <li><code>CLIENT_ID_CONFIDENTIAL</code> the id of the confidential client used for Service Account-access. This Service Account should have the following permissions:<ul> <li>update all indices in OpenSearch</li> <li>update datasets within all spaces</li> </ul> </li> <li><code>CLIENT_SECRET_CONFIDENTIAL</code> the secret of the confidential client used for Service Account-access</li> </ul>"},{"location":"operator-manual/installation/#worker","title":"Worker","text":"<p>To get an up and running instance of the workers no explicit steps are required.</p> <p>For configuration, please consider these properties within the argo-folders of the respective workers:</p> <ul> <li><code>postfix</code> a postfix you might want to add to your servides ( should be consistent across your installation as cluster-interal domains are predifined with this postfix )</li> <li><code>CONTAINER_REGISTRY</code> the container-registry that stores the Docker-image</li> <li><code>tagVersion</code> the tag-version of the Docker-image</li> </ul>"},{"location":"operator-manual/installation/#ui","title":"UI","text":"<p>Will be defined soon.</p>"},{"location":"operator-manual/prerequesits/","title":"Prerequesits (SDK ecosystem)","text":"<p>Superb Data Kraken utilizes 3rd-Party-Tools for various reasons:</p> <ul> <li>Low Implementation and Integration Effort: By incorporating 3rd-Party-Tools into the SDK, the development team can reduce the overall implementation and integration effort required. These tools are developed by experts in their respective domains and come with well-documented APIs and pre-built functionalities. Leveraging these tools enables the team to streamline the development process, saving time and resources. </li> <li>Best Possible User Experience: The integration of 3rd-Party-Tools in the SDK contributes to the delivery of an enhanced user experience. These tools are designed with a strong emphasis on user-centric principles, providing intuitive interfaces and optimized workflows. By leveraging these tools, developers can incorporate established user experience guidelines and best practices into their applications. This ensures that the end-users of the SDK have a seamless and satisfying experience, resulting in increased user satisfaction and adoption.</li> <li>Feature-Rich Implementation: 3rd-Party-Tools offer a wealth of features and functionalities that can be integrated into the SDK, enhancing its capabilities. By utilizing these tools, developers can leverage pre-built components, libraries, and APIs, reducing the need to develop functionalities from scratch. This not only saves development time but also enables the SDK to offer a wider range of features and capabilities to its users. Additionally, 3rd-Party-Tools often provide regular updates and maintenance, ensuring continuous improvement and the addition of new features, further enriching the overall implementation.</li> </ul>"},{"location":"operator-manual/prerequesits/#kubernetes","title":"Kubernetes","text":"<p>Superb Data Kraken uses Kubernetes for several reasons:</p> <ul> <li>Scalability and Resource Management Kubernetes allows for efficient scaling of applications and services. It helps manage resources effectively by automatically scaling up or down based on demand, ensuring optimal utilization of resources.</li> <li>High Availability and Fault Tolerance Kubernetes provides features like automatic container restarts, load balancing, and self-healing capabilities. These features help ensure that applications and services are highly available and can recover from failures.</li> <li>Easy Deployment and Management Kubernetes simplifies the deployment and management of applications by providing declarative configuration and automation. It allows for easy updates, rollbacks, and monitoring of applications, making it easier to manage complex systems.</li> <li>Portability and Interoperability Kubernetes is widely adopted and supported by various cloud providers and vendors. This allows for easy migration of applications across different environments and avoids vendor lock-in.</li> <li>Ecosystem and Community Support Kubernetes has a large and active community that contributes to its development and provides support. It has a rich ecosystem of tools and services that integrate well with Kubernetes, making it a popular choice for managing containerized applications.</li> </ul> <p>We use the following namespace/service-structure - to keep your adjustments to a minimum you might want to apply this structure:</p> <ul> <li><code>frontend</code><ul> <li>superb-data-kraken-frontend</li> </ul> </li> <li><code>backend</code><ul> <li>superb-data-kraken-accessmanager</li> <li>superb-data-kraken-metadata</li> <li>superb-data-kraken-organizationmanager</li> <li>superb-data-kraken-search</li> </ul> </li> <li><code>operations</code><ul> <li>superb-data-kraken-storagemanager</li> </ul> </li> <li><code>argo-mgmt</code><ul> <li>superb-data-kraken-worker</li> <li>superb-data-kraken-ingest</li> </ul> </li> </ul> <p>Global Role <code>namespace-reader</code>:</p> <pre><code>apiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: namespace-reader\nrules:\n- apiGroups:\n  - \"\"\n  - extensions\n  - apps\n  resources:\n  - configmaps\n  - pods\n  - services\n  - endpoints\n  - secrets\n  verbs:\n  - get\n  - list\n  - watch\n</code></pre>"},{"location":"operator-manual/prerequesits/#postgresql-database","title":"PostgreSQL-Database","text":"<p>You will need a PostgreSQL-Database-Server for various databases (e.g. User-Management or Organizationmanager).</p>"},{"location":"operator-manual/prerequesits/#kafka","title":"Kafka","text":"<p>SDK is in some parts event-driven. For that reason you will need a Kafka-Broker with various Topics (see Ingest or Organizationmanager).</p>"},{"location":"operator-manual/prerequesits/#azure-subscription","title":"Azure Subscription","text":"<p>As Superb Data Kraken currently supports Azure Blob Storage, you will need an Azure Subscription, that meets the requirements. These requirements consist of:</p> <ul> <li>a Service Principal with storage-access (\"storage-principal\"), i.e. the following permissions scoped on your Resource Group:<ul> <li><code>Microsoft.Storage/storageAccounts/listkeys/action</code> List Storage Account Keys</li> <li><code>Microsoft.Storage/storageAccounts/delete</code> Delete Storage Account</li> <li><code>Microsoft.Storage/storageAccounts/read</code> List/Get Storage Account(s)</li> <li><code>Microsoft.Storage/storageAccounts/write</code> Create/Update Storage Account</li> <li><code>Microsoft.Storage/storageAccounts/managementPolicies/write</code> Put storage account management policies</li> <li><code>Microsoft.Storage/storageAccounts/blobServices/write</code> Put blob service properties</li> </ul> </li> </ul> <p>Note</p> <ul> <li>You might want to use Kubernetes, PostgreSQL-Database and Kafka as managed services in Azure </li> <li>In the near future, SDK will also support other storage technologies (S3) and thus take a final step towards cloud-agnostics. Feel free to contribute </li> </ul>"},{"location":"operator-manual/prerequesits/#user-management","title":"User-Management","text":"<p>We use Keycloak as OpenID-Provider. On installing, please consider the operator-installation.</p> <p>A realm-configuration as expected by our eco-system, will be provided soon.</p>"},{"location":"operator-manual/prerequesits/#metadata-management","title":"Metadata-Management","text":"<p>We use OpenSearch for metadata management and to store analysis results. On installing, please consider the installation-guide.</p> <p>For initial setup we use this configuration (superb-data-kraken-metadata takes care of these resources in ongoing usage):</p> <ul> <li> <p><code>roles</code> - definition of a set of permissions that determine what actions a user or group of users can perform within the system:   <pre><code>_meta:\n  type: \"roles\"\n  config_version: 2\n\n# Restrict users so they can only view visualization and dashboard on kibana\nkibana_read_only:\n  reserved: true\n\n# The security REST API access role is used to assign specific users access to change the security settings through the REST API.\nsecurity_rest_api_access:\n  reserved: true\n\n# Allows users to view alerts\nalerting_view_alerts:\n  reserved: true\n  index_permissions:\n    - index_patterns:\n        - \".opendistro-alerting-alert*\"\n      allowed_actions:\n        - read\n\n# Allows users to view and acknowledge alerts\nalerting_crud_alerts:\n  reserved: true\n  index_permissions:\n    - index_patterns:\n        - \".opendistro-alerting-alert*\"\n      allowed_actions:\n        - crud\n\n# Allows users to use all alerting functionality\nalerting_full_access:\n  reserved: true\n  index_permissions:\n    - index_patterns:\n        - \".opendistro-alerting-config\"\n        - \".opendistro-alerting-alert*\"\n      allowed_actions:\n        - crud\n\nreports_read_access:\n  cluster_permissions:\n    - cluster:admin/opendistro/reports/definition/get\n    - cluster:admin/opendistro/reports/definition/list\n    - cluster:admin/opendistro/reports/instance/get\n    - cluster:admin/opendistro/reports/instance/list\n    - cluster:admin/opendistro/reports/menu/download\n</code></pre></p> </li> <li> <p><code>rolesmapping</code> - map backend roles to OpenSearch roles:   <pre><code># In this file users, backendroles and hosts can be mapped to Opensearch Security roles.\n# Permissions for Opensearch roles are configured in roles.yml\n\n_meta:\n  type: \"rolesmapping\"\n  config_version: 2\n\n# Define your roles mapping here\n\n## Demo roles mapping\n\nall_access:\n  reserved: false\n  backend_roles:\n    - \"SDK_ADMIN\"\n  description: \"Maps SDK_ADMIN to all_access\"\n\nown_index:\n  reserved: false\n  users:\n    - \"*\"\n  description: \"Allow full access to an index named like the username\"\n\nlogstash:\n  reserved: false\n  backend_roles:\n    - \"logstash\"\n\nkibana_user:\n  reserved: false\n  backend_roles:\n    - \"SDK_USER\"\n  description: \"Maps kibanauser to SDK_USER\"\n\nreports_read_access:\n  reserved: false\n  backend_roles:\n    - \"SDK_USER\"\n  description: \"Maps reports_read_access to SDK_USER\"\n\nreadall:\n  reserved: false\n  backend_roles:\n    - \"readall\"\n\nmanage_snapshots:\n  reserved: false\n  backend_roles:\n    - \"snapshotrestore\"\n\nkibana_server:\n  reserved: true\n  users:\n    - \"kibanaserver\"\n</code></pre></p> </li> <li> <p><code>tenant</code> - logical partitions or isolated spaces within the system that allow for the segregation and organization of data, resources, and configurations:   <pre><code>---\n_meta:\n  type: \"tenants\"\n  config_version: 2\n\n# Define your tenants here\n\nadmin_tenant:\n  reserved: false\n  description: \"Tenant for admin users\"\n</code></pre></p> </li> </ul>"},{"location":"operator-manual/prerequesits/#workflow-engine","title":"Workflow Engine","text":"<p>We use Argo Workflows as a workflow engine. On installing, please consider the installation-guide.</p> <p>To enable Argo Workflows to react to events (e.g. Kafka), we use Argo Events. On installing, please consider the installation-guide.</p>"},{"location":"operator-manual/prerequesits/#analysis","title":"Analysis","text":"<p>As already mentioned in Metadata-Management we use OpenSearch also as a result-store. We use OpenSearch Dashboards to visualize these results. On installing, please consider the installation-guide.</p> <p>A great addition for easily interact with your data is Jupyterhub. On installing, please consider the installation-guide.</p>"},{"location":"operator-manual/prerequesits/#monitoring","title":"Monitoring","text":"<p>Our services publish metrics to Prometheus. For a detailed installation-guide please refer to the installation-guide.</p>"},{"location":"user-guide/admin-guide/","title":"Admin Guide","text":"<p>The tasks of an administrator in the SDK include the management of organizations and spaces.</p>"},{"location":"user-guide/admin-guide/#createupdate-organization","title":"Create/update organization","text":"<p>Only users with the role <code>org_create_permission</code> are able to create organizations. If you are the owner of an Organisation, you are able to create and edit a space in it.</p> <p></p> <p>In the <code>Members</code>-tab of the add/edit mode of your organization, you are able to see the members and their roles on your organization. You are also able to add more users and corresponding roles or remove roles from users. The roles you can provide are:</p> <ul> <li>access: can access data</li> <li>admin: representative of the owner, can perform administrative activities</li> <li>trustee: can handle Dashboards on an organization-leveled basis</li> </ul> <p>For further information regarding permissions, please consider roles/rights concept.</p>"},{"location":"user-guide/admin-guide/#createupdate-space","title":"Create/update space","text":"<p>Adding a space can be done via the <code>+</code>-sign in the <code>Spaces</code>-tab within the organization-view.</p> <p></p> <p>Further space-management can be done via the pencil (edit) and trashcan (delete) icons in the upper right corner. You will get the following options when choosing to add or edit a space:</p> <ul> <li>General</li> <li>Members</li> <li>Data</li> </ul> <p></p> <p>With the <code>General</code>-tab you are able to edit the name and the description of your space. You are also able to edit the confidentiality, status and services (\"Capability\") of your space. If your space should save your data GDPR conform, you can enable this option here. But be careful, once enabled this option is permanent.</p> <p></p> <p>In the <code>Members</code>-tab of the create/edit mode of your space, you are able to see the members and their roles on your space. You are also able to add more users and corresponding roles or remove roles from users. The roles you can provide are:</p> <ul> <li>trustee: Can create data, edit and delete them</li> <li>supplier: Can create data</li> <li>user: Can read and download data</li> </ul> <p>For further information regarding permissions, please consider roles/rights concept.</p> <p></p> <p>The Data tab shows the information about how long your data will be stored and if you want a meta.json automatically created for your data. This option is handy, if you don't want to provide your own meta.json and a standard one is enough for your needs.</p> <p></p> <p>Note</p> <p>Please note that only users who have access to the organization can be added to a space.</p>"},{"location":"user-guide/howtos/","title":"How-Tos","text":""},{"location":"user-guide/howtos/#how-can-i-download-files-from-the-sdk","title":"How can I download files from the SDK?","text":""},{"location":"user-guide/howtos/#requirements","title":"Requirements","text":"<p>Install required packages:</p> <ul> <li>superb-data-klient: <code>pip install superb-data-klient</code></li> </ul>"},{"location":"user-guide/howtos/#example","title":"Example","text":"<pre><code>import superbdataklient as sdk\n\nORGANIZATION = '&lt;orga-name&gt;'\nSPACE = '&lt;space-name&gt;'\n# all files in dataset but meta.json\nREGEX = r'^&lt;root-directory-in-container&gt;\\/(?!meta\\.json$).*$'\n\nclient = sdk.SDKClient()\n\nclient.storage_download_files_with_regex(organization=ORGANIZATION, space=SPACE, local_dir='tmp', regex=REGEX)\n</code></pre>"},{"location":"user-guide/howtos/#how-can-i-create-an-analysis-index","title":"How can I create an analysis index?","text":""},{"location":"user-guide/howtos/#requirements_1","title":"Requirements","text":"<p>Install required packages:</p> <ul> <li>superb-data-klient: <code>pip install superb-data-klient</code></li> </ul>"},{"location":"user-guide/howtos/#example_1","title":"Example","text":"<pre><code>import superbdataklient as sdk\n\nORGANIZATION = '&lt;orga-name&gt;'\nSPACE = '&lt;space-name&gt;'\nROOT_DIR = '&lt;root-directory-in-container&gt;'\n\nclient = sdk.SDKClient(domain='&lt;YOUR_DOMAIN&gt;', realm='&lt;YOUR_REALM&gt;', client_id='&lt;YOUR_CLIENT&gt;', api_version='v1.0')\n\n# Body\nmappings = {}\n\nclient.application_index_create(CUSTOM_IDX_NAME, ORGANIZATION, SPACE, mappings)\n</code></pre>"},{"location":"user-guide/howtos/#how-can-document-level-security-be-avoided-by-index-level-security","title":"How can document level security be avoided by index level security?","text":""},{"location":"user-guide/howtos/#requirements_2","title":"Requirements","text":"<p>Data for different customers should be stored in Opensearch. Based on the data a dashboard shall be created.</p> <p>Customers should only see the data that belongs to them.</p>"},{"location":"user-guide/howtos/#example_2","title":"Example","text":""},{"location":"user-guide/howtos/#document-level-security","title":"Document level security","text":"<p>All documents are stored in one index in Opensearch. Document level security is applied to the index to seperate the data of different customers. This requires an attribute that contains the customer information.</p> <p>Roles and rights of a user are evaluated on each query to only return the documents that belong to the customer.</p> <p>Requires configuration of security on different levels. the Superb Data Kraken used Index-Level-Security. This has to be combined with Document-Level-Security and complicates role management.</p>"},{"location":"user-guide/howtos/#index-level-security","title":"Index level security","text":"<p>This solution is focused on using multiple indices for different customers and combining them with a common Alias.</p> <p></p> <p>The scripts ingesting data to Opensearch use an attribute in the data to decide what index shall be filled. Data for different customers are written to seperate indices. Because the Superb Data Kraken already uses Index-Level-Security no additional configuration is required.</p> <p>For querying data from all the indices a common Alias or Openasearch index-pattern is used. So there is no need to perform multiple queries on different indices.</p>"},{"location":"user-guide/quick-start/","title":"Quick start","text":"<p>Once you are logged in, you will see the welcome page of the SDK. Here you get an overview of all Organisations and  Spaces you have the rights to see. You also have the option to only show Organisations or Spaces.</p> <p>The sidebar provides some global functionality like metadata-search, Dashboards and workflows.</p> <p></p> <p>In general, however, the SDK is designed to be space-driven. For example, the search is also integrated within organizations and spaces, whereby corresponding filters are predefined.</p>"},{"location":"user-guide/walk-through/","title":"Walk Through","text":"<p>This page gives you an overview of a Use Case which contains all the standard features of the SDK. We will upload a file into the SDK and use its data to create a simple Dashboard to visualize the data.</p>"},{"location":"user-guide/walk-through/#space","title":"space","text":"<p>A space gives you the option to Upload data into the SDK and also provides you with the option to search for the uploaded files. You are also able to access your existing Dashboards or create new ones with the data you uploaded.</p> <p>Once you enter your space you will see the general options tab. Here you see:</p> <ul> <li>the space name</li> <li>the id of the space</li> <li>a short description</li> <li>the space owner</li> </ul> <p></p> <p>You have the option to upload folders or files into the SDK inside the Upload tab. You need the supplier role to be able  to upload files in the SDK (more on that later). The Upload to the SDK will take a few seconds and after that you will  be able to search the files in the Search tab. For this example we uploaded a csv file that we use later to load its data  into OpenSearch, after we created an index for them.</p> <p></p> <p>The Search tab provides you with the option to search for any word which your Upload contained. In this example we  Uploaded a file called \"Netflix Userbase\". The search result contains the metadata description of the file. You are able to access the file itself by clicking on the metadata.</p> <p></p> <p>Here you find the file itself, and you are also able to download it directly from the SDK storage.</p> <p></p> <p>The Dashboard tab provides you with the option to access your Dashboards. A detailed overview will be given later.</p> <p></p>"},{"location":"user-guide/walk-through/#python-client","title":"Python-Client","text":"<p>Now that you uploaded a file in your space, you can access the data in your file and save this data in OpenSearch,  which gets provided by the SDK. You can do that with Superb Data Klient.</p> <p>For any work with SDK you have to establish a connection. This can be done by:</p> <pre><code>DOMAIN = $(DOMAIN)\nREALM = $(REALM)\nCLIENT_ID = $(CLIENT_ID)\nUSERNAME = $(USERNAME)\nPASSWORD = $(PASSWORD)\nclient = sdk.api.SDKClient(domain=DOMAIN, realm=REALM, client_id=CLIENT_ID, username=USERNAME, password=PASSWORD)\n</code></pre> <p>You can find all versions of the login to the client here in the documentation.</p> <p>After that, you are able to use the following code snippet to download the file from the storage to your local directory:</p> <pre><code>organization = 'sdkdemonstrations'\nspace = 'netflix'\nfiles = ['20231128143642/NetflixUserbase.csv']\n\nclient.storage_download_files(organization=organization, space=space, files=files, local_dir='tmp')\n</code></pre> <p>To read the Data from the csv file you downloaded, we need pandas. </p> <p>Next you need to create an Index to upload your data to. This can be done using the SDK-API.  You have to enter the organizationName, the spaceName, a customName, the indexType and the mapping of the data in opensearch (https://opensearch.org/docs/2.11/field-types/mappings/).</p> <p>Example: </p> <pre><code>custom_name = 'users'\nmappings = {}\n\nclient.application_index_create(custom_name, organization, space, mappings)\n</code></pre> <p>After the successful installation you can use the following small code snippet to upload your Data to the OpenSearch index.</p> <pre><code>import pandas as pd\n\n# The name of the OpenSearch index the data will be uploaded to\nindex_name = f'{organization}_{space}_analysis_{custom_name}'\n\n# The File which should be uploaded\ndf = pd.read_csv('20231128143642/NetflixUserbase.csv')\n\n# Convert the pandas dataframe to a dictionary\ndocuments = df.to_dict('records')\n\n# Upload the dictionary to OpenSearch\nclient.index_documents(documents, index_name)\n</code></pre> <p>That is all you have to do for the upload. Now you can access the data in OpenSearch and create visualizations with it.</p>"},{"location":"user-guide/walk-through/#dashboards","title":"Dashboards","text":"<p>Dashboards can be accessed either through your space in the Dashboard tab or through the OpenSearch Dashboard icon on the  Sidebar.</p> <p></p> <p>For this Use Case we use OpenSearch to create a Dashboard for the Data we just uploaded. For this we first need to  create an index pattern. This is done in the <code>Dashboards Management</code> which can be found in the <code>Management</code> Tools of OpenSearch.</p> <p></p> <p>Next you have to select the option to create an Index Pattern and confirm the next two steps.</p> <p></p> <p>You now have an empty index in your OpenSearch. To confirm this, you can open the <code>Dev Tools</code> of OpenSearch (located in the <code>Management</code> section). Here you can send a query to inspect your newly created index. For this example, it would be:</p> <pre><code>GET  /sdkdemonstrations_netflix_analysis_users/_search\n</code></pre> <p>After running the python snipped and upload some data, you should get a result similar to this:</p> <p></p> <p>With this data, you are now able to use them in visualizations.</p> <p>This can be done by creating a new Dashboard.</p> <p></p> <p>In the next step you have to choose the form of the visualization best suited for your data.</p> <p></p> <p>Finally, you have to choose the index Pattern you have created.</p> <p></p> <p>Now you are able to visualize your data in a dashboard. In this simple example we took the average age per country and placed them next to each other to see which country has the oldest and which got the youngest Netflix subscribers.</p> <p></p> <p></p> <p></p> <p>Concluding, if you are creating reports for your dashboard to download, please keep in mind to mark these with the  corresponding data classification of the SDK space you are working in. To do this, add a header or footer when creating the report, like here for internal data:</p> <p></p>"}]}